Terminologies:
Label - it is the variable we're predicting, typically represented by the variable y
Features are input variables describing our data, typically represented by the variables {x1, x2, ..}


Linear regression:
Linear regression is a straight line or surface that minimizes the discrepancies between predicted and actual output values.
Basically assume x - y plane where x is the feature or input and y is the output labels. We have several points corresponding representing the predicted value corresponding to a input. Now we'll try to draw a line which best minimizes the loss i.e. assume line is  y = mx + c, so summation of (Y - y) ^ 2 is minimized. This line can be used for predicting the output for a given input. This is termed as linear regression because our model is linear
if input has many features then linear equation might depend on all the features like y = a + w1x + w2x + .. wnx

Loss:
Training a model simply means learning (determining) good values for all the weights and the bias from labeled examples. In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss; this process is called empirical risk minimization.


L2 Loss:
It is square of the difference between prediction and label = (y - y') ^ 2
In general for a large dataset L2 loss is the summation of squared losses over all the data i.e. summation( (y - y') ^ 2) over whole data

Mean Squared Error = L2 Loss / size of data set

Neural networks are highly dependent on their initialization because they do not follow a simple polynomial equation where there are lot of minimums and lot of maximums. Some minimums are better than others. So in order to reach minimum we have to initilize with values from where minimum is reachable

Gradient Descent:
To reach the minimum we need to calculate the gradient descent. to know the direction and sharpness of the step size. Note that gradient is a vector which has direction and magnitude. The gradient always points in the direction of steepest increase in the loss function. The gradient descent algorithm takes a step in the direction of the negative gradient in order to reduce loss as quickly as possible.

We could compute gradient over entire data set on each step, but this turns out to be unnecessary, computing gradient on small data samples works well which in turn saves us from huge computation.

Batch:
In gradient descent, a batch is a the set of examples you use to calculate the gradient in a single training iteration.

How to choose batch size?
A very large batch may cause even a single iteration to take a very long time to compute. A large data set with randomly sampled examples probably contains redundant data. In fact, redundancy becomes more likely as the batch size grows. Some redundancy can be useful to smooth out noisy gradients, but enormous batches tend not to carry much more predictive value than large batches.
What if we could get the right gradient on average for much less computation? By choosing examples at random from our data set, we could estimate (albeit, noisily) a big average from a much smaller one. Stochastic gradient descent (SGD) takes this idea to the extreme--it uses only a single example (a batch size of 1) per iteration

Stochastic Gradient Descent (SDG):
We take one random sample at a time.

Both SDG and entire data are not the best to compute gradient. It is best to take a optimum sized data, generally 10 - 1000 samples. This type of gradient descent is called Mini - Batch Gradient Descent

Our aim of doing machine learning is to find the best possible model in least possible time

Learning rate:
Gradient descent algorithms multiply the gradient by a scalar known as the learning rate (also sometimes called step size) to determine the next point.

Hyperparameters are the knobs that programmers modify to and fro in machine learning algorithms to provide direction and speed to the training process. Learning rate is a hyperparameter.
