Load Balancing:
Load balancing is done through hashing, every request has a ID which is fed to a hash function associated with load balancer modulus to which gives the number of the server which should be handling that request.
Since mapping is random, we expect uniform number of requests being handled by a server. 
If there are N servers and number of requests is X then requests per server is X / N, and load factor is 1 / N


Consistent hashing:
https://www.youtube.com/watch?v=zaRkONvyGr8&list=PLMCXHnjXnTnvo6alSjVkgxV-VH6EPyvoX&index=4
In consistent hashing, the hash space is divided into a ring-like structure, and each node is assigned a position on the ring using a hash function. Each data item is also hashed to determine its position on the same ring. The node responsible for storing or serving a particular data item is the next node in the ring, going clockwise from the data item's position. When the number of nodes changes, only a small fraction of the keys need to be remapped. The majority of keys remain assigned to the same nodes, which reduces the amount of data migration and provides a more efficient way to handle changes in the system's topology.


Message Queues:
Asynchronous Communication: Message queues enable asynchronous communication, meaning that producers and consumers operate independently of each other in terms of time. Producers can send messages without waiting for consumers to process them immediately, and consumers can retrieve and process messages at their own pace.
Decoupling: One of the primary benefits of message queues is decoupling. Producers and consumers are decoupled in time and space, meaning they don't need to be aware of each other's existence. This makes it easier to scale, maintain, and evolve the different parts of a distributed system independently.
Fault Tolerance: Message queues can contribute to fault tolerance by allowing messages to be stored in the queue even if a consumer is temporarily unavailable. Once the consumer is back online, it can retrieve and process the messages.


Monolithic Architecture:
Single, Unified Codebase:
In a monolithic architecture, the entire application is developed, deployed, and maintained as a single, unified codebase.
All components, modules, and functionalities are tightly integrated into a single executable.
Tight Coupling:
Components within the monolith are typically tightly coupled, meaning changes to one part of the system may have cascading effects on other parts.
Development, testing, and deployment are often done as a single unit.
Scalability:
Scaling a monolithic application involves replicating the entire application, even if only a specific part needs more resources.
Vertical scaling (adding more resources to a single server) is a common approach.
Technology Stack:
A monolith often uses a single technology stack and framework.
Development and Deployment:
Development and deployment are centralized, with all features and updates released together.


Microservices Architecture:
Decentralized and Distributed:
Microservices architecture involves breaking down the application into small, independent services that communicate with each other over a network.
Each service is a separate and deployable unit.
Loose Coupling:
Services in a microservices architecture are loosely coupled, allowing for independent development, deployment, and scaling.
Changes to one service do not necessarily affect others.
Scalability:
Microservices support horizontal scaling, where individual services can be scaled independently based on specific needs.
Each service can be developed and scaled independently.
Technology Stack:
Different services within a microservices architecture can use different technology stacks, frameworks, and databases.
Teams can choose the best tools for each specific service.
Development and Deployment:
Development and deployment are decentralized, with each service having its own development and deployment lifecycle.
Enables more frequent and independent releases.
Fault Isolation:
If a microservice fails, it doesn't necessarily bring down the entire system. Fault isolation is inherent, as other services can continue to function.
Complexity:
While microservices offer flexibility and scalability, they also introduce complexities in terms of network communication, data consistency, and orchestration.



Distributed caches are a type of caching system that spans multiple servers or nodes in a network. Caching involves storing frequently used data in a quickly accessible location to reduce the time and resources needed to retrieve that data when it is requested again. Distributed caching extends this concept across multiple machines, providing a scalable and high-performance solution for caching in large and distributed systems.
Maintaining data consistency across distributed nodes is a challenge in distributed systems. Distributed cache systems often implement strategies to ensure data consistency, such as cache invalidation, time-to-live (TTL) policies, or using a distributed cache coherence protocol.